{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81ee5146-435c-4c36-b3b6-6f09c60765f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 200, 200, 3) (10222, 0)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import pandas as pd\n",
    "from os import makedirs\n",
    "from numpy import load\n",
    "from random import seed\n",
    "from random import random\n",
    "from shutil import copyfile\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fd428-cdca-4b4c-870e-bb0370ae829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('./labels.csv')\n",
    "# labels_df.rename(columns={'id': 'name'}, inplace=True)\n",
    "# define location of dataset\n",
    "folder = './train/'\n",
    "photos, labels = list(), list()\n",
    "# enumerate files in the directory\n",
    "for file in listdir(folder):\n",
    "    filename = file.rstrip('.jpg')\n",
    "    # print(labels_df.loc[labels_df['id'] == filename, ['breed']])\n",
    "\t# load image\n",
    "    photo = load_img(folder + file, target_size=(200, 200))\n",
    "    # convert to numpy array\n",
    "    photo = img_to_array(photo)\n",
    "    # store\n",
    "    photos.append(photo)\n",
    "    labels.append(labels_df.loc[labels_df['id'] == file]['breed'])\n",
    "# convert to a numpy arrays\n",
    "photos = asarray(photos)\n",
    "labels = asarray(labels)\n",
    "print(photos.shape, labels.shape)\n",
    "# save the reshaped photos\n",
    "save('dogs_breeds_photos.npy', photos)\n",
    "save('dogs_breeds_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4bf849-b9fa-4279-b71c-34a228630da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 200, 200, 3) (10222, 0)\n"
     ]
    }
   ],
   "source": [
    "# load and confirm the shape\n",
    "from numpy import load\n",
    "photos = load('dogs_breeds_photos.npy', allow_pickle=True)\n",
    "labels = load('dogs_breeds_labels.npy', allow_pickle=True)\n",
    "print(photos.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d97145-371d-43c7-accc-53170df0033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "\n",
    "labels_df = pd.read_csv('./labels.csv')\n",
    "labels_list = labels_df['breed'].values\n",
    "# create directories\n",
    "dataset_home = 'dataset_dog_breeds/'\n",
    "subdirs = ['train/', 'test/']\n",
    "for subdir in subdirs:\n",
    "    # create label subdirectories\n",
    "    for labldir in labels_list:\n",
    "        labldir = labldir + '/'\n",
    "        newdir = dataset_home + subdir + labldir\n",
    "        makedirs(newdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb8c7693-087d-4720-8714-36670a226223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "from shutil import copyfile\n",
    "\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "# define ratio of pictures to use for validation\n",
    "val_ratio = 0.25\n",
    "# copy training dataset images into subdirectories\n",
    "src_directory = 'train/'\n",
    "for file in listdir(src_directory):\n",
    "    src = src_directory + '/' + file\n",
    "    name = file.rstrip('.jpg')\n",
    "    dst_dir = 'train/'\n",
    "    if random() < val_ratio:\n",
    "        dst_dir = 'test/'\n",
    "    breed_dir = labels_df.loc[labels_df['id'] == name]['breed'].values + '/'\n",
    "    # if file.startswith('cat'):\n",
    "    #     dst = dataset_home + dst_dir + 'cats/'  + file\n",
    "    #     copyfile(src, dst)\n",
    "    # elif file.startswith('dog'):\n",
    "    #     dst = dataset_home + dst_dir + 'dogs/'  + file\n",
    "    #     copyfile(src, dst)\n",
    "    dst = dataset_home + dst_dir + breed_dir[0]  + file\n",
    "    copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008fb60d-4559-4c8b-93a1-6dbc6c62b7cd",
   "metadata": {},
   "source": [
    "relu\n",
    "pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc61d3d2-a2aa-4268-9d2e-604bf13760c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# baseline model for the dogs vs cats dataset\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    " \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    filename = 'wynik'\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    # pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c97a7f9a-98f8-4d8f-9e15-a08a21c5d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7598 images belonging to 120 classes.\n",
      "Found 2624 images belonging to 120 classes.\n",
      "> 0.800\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    # model = Sequential()\n",
    "    # model.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    # return model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, input_dim=120, activation='relu', kernel_initializer='he_uniform'))\n",
    "    # model.add(Dropout(0.3))\n",
    "    # model.add(Dense(120, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = define_model()\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# prepare iterators\n",
    "train_it = datagen.flow_from_directory('dataset_dog_breeds/train/',\n",
    "                                       class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "test_it = datagen.flow_from_directory('dataset_dog_breeds/test/',\n",
    "                                      class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "# fit model\n",
    "history = model.fit(train_it, steps_per_epoch=len(train_it),\n",
    "                    validation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)\n",
    "# evaluate model\n",
    "_, acc = model.evaluate(test_it, steps=len(test_it), verbose=0)\n",
    "print('> %.3f' % (acc * 100.0))\n",
    "# learning curves\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85473f-416a-4fef-9ed8-7f7c4411095a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f6c7f0-52fa-41a3-b67d-04f2c1f03fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7598 images belonging to 120 classes.\n",
      "Found 2624 images belonging to 120 classes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AJOZEF~1\\AppData\\Local\\Temp/ipykernel_22156/12256545.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \tclass_mode='binary', batch_size=64, target_size=(200, 200))\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m history = model2.fit(train_it, steps_per_epoch=len(train_it),\n\u001b[0m\u001b[0;32m     27\u001b[0m \tvalidation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ajozefowicz\\pycharmprojects\\racedetector\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model2 = define_model()\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# prepare iterators\n",
    "train_it = datagen.flow_from_directory('dataset_dog_breeds/train/',\n",
    "    class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "test_it = datagen.flow_from_directory('dataset_dog_breeds/test/',\n",
    "    class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "# fit model\n",
    "history = model2.fit(train_it, steps_per_epoch=len(train_it),\n",
    "    validation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)\n",
    "# evaluate model\n",
    "_, acc = model2.evaluate(test_it, steps=len(test_it), verbose=0)\n",
    "print('> %.3f' % (acc * 100.0))\n",
    "# learning curves\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "362ade72-54d0-4cf9-94ea-ff763aab9ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "folder = './Fred/'\n",
    "file = 'test.jpg'\n",
    "photo = load_img(folder + file, target_size=(200, 200))\n",
    "# convert to numpy array\n",
    "photo = img_to_array(photo)\n",
    "data = []\n",
    "data.append(photo)\n",
    "data = np.array(data)\n",
    "model.predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
